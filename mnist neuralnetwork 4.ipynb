{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(784,30)\n",
    "        self.fc2=nn.Linear(30,10)\n",
    "    def forward(self,x):\n",
    "        l1=self.fc1(x)\n",
    "        al1=torch.sigmoid(l1)\n",
    "        l2=self.fc2(al1)\n",
    "        al2=torch.sigmoid(l2)\n",
    "        return al2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,use_cuda,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "    for batchid,(data,target) in enumerate(train_loader):\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "        if use_cuda:\n",
    "            data,y_onehot=data.cuda(),y_onehot.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=torch.mean((output-y_onehot)**2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batchid % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batchid * len(data), len(train_loader.dataset),\n",
    "            100. * batchid / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,use_cuda,test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data=data.reshape([data.shape[0],784])\n",
    "            y_onehot=torch.zeros([target.shape[0],10])\n",
    "            y_onehot[range(target.shape[0]),target]=1\n",
    "            if use_cuda:\n",
    "                data,y_onehot=data.cuda(),y_onehot.cuda()\n",
    "            output=model(data)\n",
    "            test_loss+=torch.sum((output-y_onehot)**2)\n",
    "            pred=output.argmax(dim=1,keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
    "        test_loss/=len(test_loader.dataset)\n",
    "        print(test_loss,100*correct/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed_value):\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda=False\n",
    "    seed(0)\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ])\n",
    "    data1=datasets.MNIST('../data',download=True,train=True,transform=transform)\n",
    "    data2=datasets.MNIST('../data',train=False,transform=transform)\n",
    "    train_loader=torch.utils.data.DataLoader(data1,num_workers=6,batch_size=64,shuffle=True)\n",
    "    test_loader=torch.utils.data.DataLoader(data2,num_workers=6,batch_size=1000,shuffle=False)\n",
    "    model=Net()\n",
    "    if use_cuda:\n",
    "        model=model.cuda()\n",
    "    optimizer=optim.SGD(model.parameters(),lr=10)\n",
    "    for epoch in range(1,11):\n",
    "        train(model,use_cuda,train_loader,optimizer,epoch)\n",
    "        test(model,use_cuda,test_loader)\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
    "    model.load_state_dict(torch.load(\"mnist_cnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.284883\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.033432\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.021424\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.014365\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.013061\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.012427\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.015567\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.019503\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.025899\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.012034\n",
      "tensor(0.1259) 92.86\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.015895\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.007555\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.015096\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.018973\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.015171\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.012200\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.014843\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.014440\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.008070\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.017088\n",
      "tensor(0.1134) 93.35\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.008015\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.017204\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.011106\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.012995\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.007430\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.020739\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.007522\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.013110\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.007818\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.003213\n",
      "tensor(0.1013) 93.98\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.016565\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.014961\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.013758\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.008941\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.011942\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.007298\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.003900\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.011472\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.008680\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.014769\n",
      "tensor(0.0971) 94.24\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003981\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.008105\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.009571\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008531\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.012194\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.005845\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.003714\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.010216\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.006352\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.004115\n",
      "tensor(0.0919) 94.62\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007999\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.003582\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.005937\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005832\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006726\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.007673\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.009174\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.018349\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.008916\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.008805\n",
      "tensor(0.0910) 94.58\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.010705\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.008210\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.009228\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010791\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.007346\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.012426\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.006190\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004888\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004666\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.009560\n",
      "tensor(0.0879) 94.7\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.004995\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.004099\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.007337\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.005843\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005839\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.010339\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.005909\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.012023\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.007523\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.009189\n",
      "tensor(0.0875) 94.74\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.010473\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004499\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009012\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003830\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.005452\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.011064\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.006081\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.004221\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.005610\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.002737\n",
      "tensor(0.0862) 94.97\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.005415\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.010509\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.005977\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.011389\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.006678\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.012694\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.006448\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.003650\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.007402\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.005559\n",
      "tensor(0.0858) 95.0\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.276752\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.025827\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.012155\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.020065\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.011308\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.012245\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.016315\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.020948\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.011743\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.007604\n",
      "\n",
      "Test set: Average loss: 0.1007, Accuracy: 9388/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.008053\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.015624\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.008314\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.014319\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.010612\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.002161\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.013032\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.007575\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.013966\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.003200\n",
      "\n",
      "Test set: Average loss: 0.0768, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.011564\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.003401\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.003523\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.002111\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.003047\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.007964\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.002057\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.008780\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.006561\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.003866\n",
      "\n",
      "Test set: Average loss: 0.0662, Accuracy: 9617/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.007791\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.002414\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.009939\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.007050\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.004067\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006429\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.006031\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007068\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.006241\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.003196\n",
      "\n",
      "Test set: Average loss: 0.0599, Accuracy: 9661/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003045\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.008133\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.010227\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004135\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.005522\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004656\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.004885\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005240\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.006568\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000616\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.002448\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.005636\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.011477\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.003323\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000878\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.002389\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.004150\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.006706\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.002023\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.008738\n",
      "\n",
      "Test set: Average loss: 0.0544, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000986\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.000732\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004871\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001263\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.004486\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001467\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.005851\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001070\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.004142\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.008066\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.004411\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001515\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.003735\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002364\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003904\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001032\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.010042\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.000869\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.002492\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001220\n",
      "\n",
      "Test set: Average loss: 0.0503, Accuracy: 9716/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002673\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004733\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.002538\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003835\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.000702\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003398\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.003866\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001552\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.003330\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.005818\n",
      "\n",
      "Test set: Average loss: 0.0483, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.004838\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001719\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.008512\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001760\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000971\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001462\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001269\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000982\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001380\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.003572\n",
      "\n",
      "Test set: Average loss: 0.0474, Accuracy: 9739/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):  # Constructor\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 2 Layers\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "        # xavier_initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - shape = [64, 784]\n",
    "        # Layer 1\n",
    "        l1 = self.fc1(x)  # l1 - shape = [64, 100]\n",
    "\n",
    "        # Activation 1\n",
    "        l1_a1 = torch.sigmoid(l1)  # l1_a1 - shape = [64, 100]\n",
    "\n",
    "        # Layer 2\n",
    "        l2 = self.fc2(l1_a1)  # l2 - shape = [64, 10]\n",
    "\n",
    "        # Activation 2\n",
    "        l2_a2 = torch.sigmoid(l2)  # l2_a2 - shape = [64, 10]\n",
    "        \n",
    "        return l2_a2\n",
    "\n",
    "\n",
    "def train(model, use_cuda, train_loader, optimizer, epoch):\n",
    "\n",
    "    model.train()  # Tell the model to prepare for training\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Get the batch\n",
    "\n",
    "        # Converting the target to one-hot-encoding from categorical encoding\n",
    "        # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [64, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1\n",
    "\n",
    "        data = data.view([data.shape[0], 784])\n",
    "\n",
    "        if use_cuda:\n",
    "            data, y_onehot = data.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Setting the cumulative gradients to 0\n",
    "        output = model(data)  # Forward pass through the model\n",
    "        loss = torch.mean((output - y_onehot)**2)  # Calculating the loss\n",
    "        loss.backward()  # Calculating the gradients of the model. Note that the model has not yet been updated.\n",
    "        optimizer.step()  # Updating the model parameters. Note that this does not remove the stored gradients!\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, use_cuda, test_loader):\n",
    "\n",
    "    model.eval()  # Tell the model to prepare for testing or evaluation\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # Tell the model that gradients need not be calculated\n",
    "        for data, target in test_loader:  # Get the batch\n",
    "\n",
    "            # Converting the target to one-hot-encoding from categorical encoding\n",
    "            # Converting the data to [batch_size, 784] from [batch_size, 1, 28, 28]\n",
    "\n",
    "            y_onehot = torch.zeros([target.shape[0], 10])\n",
    "            y_onehot[range(target.shape[0]), target] = 1\n",
    "            data = data.view([data.shape[0], 784])\n",
    "\n",
    "            if use_cuda:\n",
    "                data, target, y_onehot = data.cuda(), target.cuda(), y_onehot.cuda()  # Sending the data to the GPU\n",
    "\n",
    "            # argmax([0.1, 0.2, 0.9, 0.4]) => 2\n",
    "            # output - shape = [1000, 10], argmax(dim=1) => [1000]\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += torch.sum((output - y_onehot)**2)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Get total number of correct samples\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Accuracy = Total Correct / Total Samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def seed(seed_value):\n",
    "\n",
    "    # This removes randomness, makes everything deterministic\n",
    "\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    use_cuda = False  # Set it to False if you are using a CPU\n",
    "    # Colab And Kaggle\n",
    "\n",
    "    seed(0)  # Used to fix randomness in the code! Very important!\n",
    "\n",
    "    # Convert the dataset to tensor and subtract the mean and divide by standard\n",
    "    # deviation. Why? So that neurons don't saturate!\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # ((mean), (std))    (x - mean)/std\n",
    "    ])\n",
    "    # [a1, a2, a3] / [b1, b2, b3] = [a1/b1, a2/b2, a3/b3]\n",
    "\n",
    "    # Wx + b =~ b\n",
    "\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)  # Get the train dataset\n",
    "    dataset2 = datasets.MNIST('../data', train=False, transform=transform)  # Get the test dataset\n",
    "\n",
    "    # Get the train data-loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, num_workers=6, shuffle=True, batch_size=64)\n",
    "    # Get the test data-loader\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, num_workers=6, shuffle=False, batch_size=1000)\n",
    "\n",
    "    model = Net()  # Get the model\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Put the model weights on GPU\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=10)  # Choose the optimizer and the set the learning rate\n",
    "\n",
    "    for epoch in range(1, 10 + 1):\n",
    "        train(model, use_cuda, train_loader, optimizer, epoch)  # Train the network\n",
    "        test(model, use_cuda, test_loader)  # Test the network\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnist.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist.pt'))\n",
    "\n",
    "    # Loading a saved model - model.load_state_dict(torch.load('mnist_cnn.pt'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c5c00c7017fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
