{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cooking.stackexchange.txt', 'r')\n",
    "# read all text\n",
    "text = file.readlines()\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'sauce ',\n",
       " 'cheese How much does potato starch affect a cheese sauce recipe?\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0].split(\"__label__\")# here we can observe that first item in list is always going to be empty string and one of the labels is attached to the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text-Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15404/15404 [00:00<00:00, 123357.17it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences=[]#to store all sentences in the corpus\n",
    "all_label=set()#to store all the unique labels\n",
    "labels=[]# to store the corresponding label to a sentence \n",
    "\n",
    "for i in tqdm(text):#separating labels and sentences of each line\n",
    "    p=i.strip().split(\"__label__\")\n",
    "    p.pop(0)\n",
    "    k=p.pop().split(\" \")#k contains sentence\n",
    "    p.append(k.pop(0))#p contains all the labels \n",
    "    \n",
    "    for i in range(len(p)):\n",
    "        p[i]=p[i].strip()\n",
    "        all_label.add(p[i])\n",
    "    sentences.append(''.join([i.lower()+\" \" for i in k]))\n",
    "    labels.append(p)\n",
    "all_label=list(all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):# pre processing the sentences\n",
    "    \n",
    "    nopunc = [char for char in mess if char not in string.punctuation]#removing punctuations\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #removing stopwords and returning the list of words in a sentence without punctuation and stopwords.\n",
    "    return [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15404/15404 [00:27<00:00, 562.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(sentences))):# preprocessing all sentences in the corpus\n",
    "    sentences[i]=(text_process(sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9469\n"
     ]
    }
   ],
   "source": [
    "#Creating a list of unique words in all sentences\n",
    "total_words=set()\n",
    "for sent in sentences:\n",
    "    for t in sent:\n",
    "        total_words.add(t)\n",
    "print(len(total_words))\n",
    "total_words=list(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15404/15404 [00:46<00:00, 330.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#creating bag of words vectors for all sentences\n",
    "BOW=[]\n",
    "for sent in tqdm(sentences):\n",
    "    sentence_bow_vector=[]\n",
    "    for t in total_words:\n",
    "        if t in sent:\n",
    "            sentence_bow_vector.append(sent.count(t))\n",
    "        else:\n",
    "            sentence_bow_vector.append(0)\n",
    "    BOW.append(sentence_bow_vector)\n",
    "BOW=np.array(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15404, 9469)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15404/15404 [00:00<00:00, 116009.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#converting the labels into onehot encoded vectors\n",
    "label_index={j:i for i,j in enumerate(all_label)}\n",
    "label_onehot=np.zeros([len(labels),736])\n",
    "for i in tqdm(range(len(labels))):\n",
    "    for k in labels[i]:\n",
    "        label_onehot[i][label_index[k]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15404, 736)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing BOW vectors\n",
    "BOW=preprocessing.normalize(BOW)\n",
    "BOW=BOW.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying principal component analysis for dimensionality reduction\n",
    "pca=PCA(n_components=500)\n",
    "pca.fit(BOW)\n",
    "BOW=pca.transform(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15404, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into testing and training datasets\n",
    "sent_train,sent_test,label_train,label_test=train_test_split(BOW,label_onehot,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a linear regression model on the training dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LR_model=LinearRegression()\n",
    "LR_model.fit(sent_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=LR_model.predict(sent_test)#getting predictions given by the model on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.32976306394028\n"
     ]
    }
   ],
   "source": [
    "#calculating accuracy of the model\n",
    "correct=0\n",
    "for i in range(len(predictions)):\n",
    "    k=np.argmax(predictions[i])\n",
    "    if label_test[i][k]==1:\n",
    "        correct+=1\n",
    "print(100*correct/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning model using pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self,sent,lab):\n",
    "        self.sent=sent\n",
    "        self.lab=lab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lab)\n",
    "    def __getitem__(self,idx):\n",
    "        label=self.lab[idx]\n",
    "        sentence=self.sent[idx]\n",
    "        \n",
    "        return sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        # input layer of 500 neurons, hidden layer of 100 neurons and a output layer of 736 neurons\n",
    "        self.fc1=nn.Linear(500,100)\n",
    "        self.fc2=nn.Linear(100,736)\n",
    "        self.dropout=nn.Dropout(0.2)# a dropout layer dropping 20 percent randomly selected neurons\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x-shape-> [batch_size,500]\n",
    "        #layer-1\n",
    "        l1=self.fc1(x)#l1 shape->[batch_size,100]\n",
    "        #activation-function-1\n",
    "        al1=F.relu(l1)#al1 shaoe->[batch_size,100]\n",
    "        #dropout layer\n",
    "        al1=self.dropout(al1)\n",
    "        #layer-2\n",
    "        l2=self.fc2(al1)#l2.shape->[batch_size,736]\n",
    "        \n",
    "        return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,optimizer,epoch):\n",
    "    \n",
    "    model.train()# telling the model to prapre for training\n",
    "    \n",
    "    for batchid,(data,target) in enumerate(train_loader): # getting the batch\n",
    "        \n",
    "        #converting the target to categorical variable\n",
    "        y_categorical=target.argmax(dim=1,keepdim=True)\n",
    "        y_categorical=torch.flatten(y_categorical)\n",
    "        \n",
    "        optimizer.zero_grad()#setting the cummulative gradients to zero\n",
    "        output=model(data)#forward pass through the model\n",
    "        \n",
    "        loss=F.cross_entropy(output,y_categorical)# this function applies softmax activation function and then logloss\n",
    "        loss.backward()#calculating gradients of the model\n",
    "        optimizer.step()#updating model parameters\n",
    "        \n",
    "        if batchid % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batchid * len(data), len(train_loader.dataset),\n",
    "            100. * batchid / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader):\n",
    "    \n",
    "    model.eval()#telling the model to preapre for evaluation\n",
    "    \n",
    "    correct=0#variable to store total correct predictions\n",
    "    \n",
    "    with torch.no_grad():#to ensure gradients are not calculated as calculating gradients is not required for testing\n",
    "        \n",
    "        for data,target in test_loader:#getting the batch\n",
    "            \n",
    "            output=model(data)#forward pass shape->[batch_size,736]\n",
    "            pred=output.argmax(dim=1,keepdim=True)#getting the index of max value in the prediction. shape->[batch_size,1]\n",
    "            \n",
    "            for i in range(len(pred)):\n",
    "                if target[i][pred[i]]==1:# checking if the predicted value is one of the targets\n",
    "                    correct+=1\n",
    "        print(100*correct/len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed_value):\n",
    "    #this function removes randomness and makes everything deterministic\n",
    "    #here we set the seed for torch.cuda,torch,numpy and random.\n",
    "    #torch.cuda.manual_seed_all(seed_value) ,if we are using multi-GPU then we should use this to set the seed.\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    seed(0)#fixing the randomness of the code\n",
    "    \n",
    "    #passing the data into custom data loader\n",
    "    data1=customdataloader(sent_train,label_train)\n",
    "    data2=customdataloader(sent_test,label_test)\n",
    "    \n",
    "    train_loader=torch.utils.data.DataLoader(data1,num_workers=0,batch_size=30,shuffle=True)#getting train data loader\n",
    "    test_loader=torch.utils.data.DataLoader(data2,num_workers=0,batch_size=50,shuffle=False)#getting test data loader\n",
    "    \n",
    "    model=Net()\n",
    "    \n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.001)#choosing the optimizer and setting the learning rate\n",
    "    \n",
    "    for epoch in range(1,26):\n",
    "        train(model,train_loader,optimizer,epoch)\n",
    "        test(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/12323 (0%)]\tLoss: 6.590783\n",
      "Train Epoch: 1 [3000/12323 (24%)]\tLoss: 6.097587\n",
      "Train Epoch: 1 [6000/12323 (49%)]\tLoss: 5.179250\n",
      "Train Epoch: 1 [9000/12323 (73%)]\tLoss: 5.543921\n",
      "Train Epoch: 1 [12000/12323 (97%)]\tLoss: 5.869801\n",
      "13.209996754300551\n",
      "Train Epoch: 2 [0/12323 (0%)]\tLoss: 5.412152\n",
      "Train Epoch: 2 [3000/12323 (24%)]\tLoss: 5.492341\n",
      "Train Epoch: 2 [6000/12323 (49%)]\tLoss: 4.656296\n",
      "Train Epoch: 2 [9000/12323 (73%)]\tLoss: 5.480835\n",
      "Train Epoch: 2 [12000/12323 (97%)]\tLoss: 4.698659\n",
      "30.931515741642325\n",
      "Train Epoch: 3 [0/12323 (0%)]\tLoss: 4.863535\n",
      "Train Epoch: 3 [3000/12323 (24%)]\tLoss: 4.633924\n",
      "Train Epoch: 3 [6000/12323 (49%)]\tLoss: 3.603258\n",
      "Train Epoch: 3 [9000/12323 (73%)]\tLoss: 4.593738\n",
      "Train Epoch: 3 [12000/12323 (97%)]\tLoss: 4.897063\n",
      "38.78610840636157\n",
      "Train Epoch: 4 [0/12323 (0%)]\tLoss: 4.734938\n",
      "Train Epoch: 4 [3000/12323 (24%)]\tLoss: 4.091980\n",
      "Train Epoch: 4 [6000/12323 (49%)]\tLoss: 4.539934\n",
      "Train Epoch: 4 [9000/12323 (73%)]\tLoss: 4.999915\n",
      "Train Epoch: 4 [12000/12323 (97%)]\tLoss: 4.381170\n",
      "42.648490749756576\n",
      "Train Epoch: 5 [0/12323 (0%)]\tLoss: 3.980530\n",
      "Train Epoch: 5 [3000/12323 (24%)]\tLoss: 3.824907\n",
      "Train Epoch: 5 [6000/12323 (49%)]\tLoss: 4.250654\n",
      "Train Epoch: 5 [9000/12323 (73%)]\tLoss: 4.246358\n",
      "Train Epoch: 5 [12000/12323 (97%)]\tLoss: 4.850858\n",
      "47.127555988315486\n",
      "Train Epoch: 6 [0/12323 (0%)]\tLoss: 3.579371\n",
      "Train Epoch: 6 [3000/12323 (24%)]\tLoss: 3.764545\n",
      "Train Epoch: 6 [6000/12323 (49%)]\tLoss: 3.887497\n",
      "Train Epoch: 6 [9000/12323 (73%)]\tLoss: 3.709233\n",
      "Train Epoch: 6 [12000/12323 (97%)]\tLoss: 3.505931\n",
      "49.33463161311263\n",
      "Train Epoch: 7 [0/12323 (0%)]\tLoss: 3.164101\n",
      "Train Epoch: 7 [3000/12323 (24%)]\tLoss: 4.018212\n",
      "Train Epoch: 7 [6000/12323 (49%)]\tLoss: 4.222211\n",
      "Train Epoch: 7 [9000/12323 (73%)]\tLoss: 4.140131\n",
      "Train Epoch: 7 [12000/12323 (97%)]\tLoss: 3.013566\n",
      "51.0548523206751\n",
      "Train Epoch: 8 [0/12323 (0%)]\tLoss: 3.675102\n",
      "Train Epoch: 8 [3000/12323 (24%)]\tLoss: 3.710548\n",
      "Train Epoch: 8 [6000/12323 (49%)]\tLoss: 3.010202\n",
      "Train Epoch: 8 [9000/12323 (73%)]\tLoss: 3.357282\n",
      "Train Epoch: 8 [12000/12323 (97%)]\tLoss: 4.218269\n",
      "52.64524505030834\n",
      "Train Epoch: 9 [0/12323 (0%)]\tLoss: 4.011724\n",
      "Train Epoch: 9 [3000/12323 (24%)]\tLoss: 3.184761\n",
      "Train Epoch: 9 [6000/12323 (49%)]\tLoss: 2.549733\n",
      "Train Epoch: 9 [9000/12323 (73%)]\tLoss: 2.475037\n",
      "Train Epoch: 9 [12000/12323 (97%)]\tLoss: 3.100430\n",
      "53.29438493995456\n",
      "Train Epoch: 10 [0/12323 (0%)]\tLoss: 3.506766\n",
      "Train Epoch: 10 [3000/12323 (24%)]\tLoss: 2.803896\n",
      "Train Epoch: 10 [6000/12323 (49%)]\tLoss: 3.542150\n",
      "Train Epoch: 10 [9000/12323 (73%)]\tLoss: 3.448975\n",
      "Train Epoch: 10 [12000/12323 (97%)]\tLoss: 3.304505\n",
      "53.74878286270691\n",
      "Train Epoch: 11 [0/12323 (0%)]\tLoss: 3.142726\n",
      "Train Epoch: 11 [3000/12323 (24%)]\tLoss: 2.781426\n",
      "Train Epoch: 11 [6000/12323 (49%)]\tLoss: 2.741509\n",
      "Train Epoch: 11 [9000/12323 (73%)]\tLoss: 2.089460\n",
      "Train Epoch: 11 [12000/12323 (97%)]\tLoss: 3.216383\n",
      "54.52775073028238\n",
      "Train Epoch: 12 [0/12323 (0%)]\tLoss: 2.551412\n",
      "Train Epoch: 12 [3000/12323 (24%)]\tLoss: 2.873313\n",
      "Train Epoch: 12 [6000/12323 (49%)]\tLoss: 3.646750\n",
      "Train Epoch: 12 [9000/12323 (73%)]\tLoss: 2.617488\n",
      "Train Epoch: 12 [12000/12323 (97%)]\tLoss: 2.572630\n",
      "54.46283674131775\n",
      "Train Epoch: 13 [0/12323 (0%)]\tLoss: 2.592166\n",
      "Train Epoch: 13 [3000/12323 (24%)]\tLoss: 1.727253\n",
      "Train Epoch: 13 [6000/12323 (49%)]\tLoss: 2.125787\n",
      "Train Epoch: 13 [9000/12323 (73%)]\tLoss: 3.345321\n",
      "Train Epoch: 13 [12000/12323 (97%)]\tLoss: 2.662717\n",
      "54.75494969165855\n",
      "Train Epoch: 14 [0/12323 (0%)]\tLoss: 1.948932\n",
      "Train Epoch: 14 [3000/12323 (24%)]\tLoss: 3.199369\n",
      "Train Epoch: 14 [6000/12323 (49%)]\tLoss: 2.630455\n",
      "Train Epoch: 14 [9000/12323 (73%)]\tLoss: 2.548280\n",
      "Train Epoch: 14 [12000/12323 (97%)]\tLoss: 2.876287\n",
      "54.75494969165855\n",
      "Train Epoch: 15 [0/12323 (0%)]\tLoss: 2.307078\n",
      "Train Epoch: 15 [3000/12323 (24%)]\tLoss: 2.279567\n",
      "Train Epoch: 15 [6000/12323 (49%)]\tLoss: 2.033991\n",
      "Train Epoch: 15 [9000/12323 (73%)]\tLoss: 1.927164\n",
      "Train Epoch: 15 [12000/12323 (97%)]\tLoss: 2.899698\n",
      "55.11197663096397\n",
      "Train Epoch: 16 [0/12323 (0%)]\tLoss: 3.258833\n",
      "Train Epoch: 16 [3000/12323 (24%)]\tLoss: 2.584086\n",
      "Train Epoch: 16 [6000/12323 (49%)]\tLoss: 3.117843\n",
      "Train Epoch: 16 [9000/12323 (73%)]\tLoss: 2.061960\n",
      "Train Epoch: 16 [12000/12323 (97%)]\tLoss: 2.523757\n",
      "54.98214865303473\n",
      "Train Epoch: 17 [0/12323 (0%)]\tLoss: 2.649681\n",
      "Train Epoch: 17 [3000/12323 (24%)]\tLoss: 2.972181\n",
      "Train Epoch: 17 [6000/12323 (49%)]\tLoss: 1.889460\n",
      "Train Epoch: 17 [9000/12323 (73%)]\tLoss: 2.344280\n",
      "Train Epoch: 17 [12000/12323 (97%)]\tLoss: 2.789737\n",
      "55.047062641999354\n",
      "Train Epoch: 18 [0/12323 (0%)]\tLoss: 2.131886\n",
      "Train Epoch: 18 [3000/12323 (24%)]\tLoss: 2.965992\n",
      "Train Epoch: 18 [6000/12323 (49%)]\tLoss: 3.035082\n",
      "Train Epoch: 18 [9000/12323 (73%)]\tLoss: 2.711863\n",
      "Train Epoch: 18 [12000/12323 (97%)]\tLoss: 2.502368\n",
      "55.047062641999354\n",
      "Train Epoch: 19 [0/12323 (0%)]\tLoss: 2.766374\n",
      "Train Epoch: 19 [3000/12323 (24%)]\tLoss: 1.584437\n",
      "Train Epoch: 19 [6000/12323 (49%)]\tLoss: 2.698762\n",
      "Train Epoch: 19 [9000/12323 (73%)]\tLoss: 2.663620\n",
      "Train Epoch: 19 [12000/12323 (97%)]\tLoss: 2.342936\n",
      "54.94969165855242\n",
      "Train Epoch: 20 [0/12323 (0%)]\tLoss: 1.629722\n",
      "Train Epoch: 20 [3000/12323 (24%)]\tLoss: 3.069364\n",
      "Train Epoch: 20 [6000/12323 (49%)]\tLoss: 2.263821\n",
      "Train Epoch: 20 [9000/12323 (73%)]\tLoss: 1.957715\n",
      "Train Epoch: 20 [12000/12323 (97%)]\tLoss: 2.270203\n",
      "54.56020772476469\n",
      "Train Epoch: 21 [0/12323 (0%)]\tLoss: 3.064708\n",
      "Train Epoch: 21 [3000/12323 (24%)]\tLoss: 2.492546\n",
      "Train Epoch: 21 [6000/12323 (49%)]\tLoss: 2.153032\n",
      "Train Epoch: 21 [9000/12323 (73%)]\tLoss: 1.508150\n",
      "Train Epoch: 21 [12000/12323 (97%)]\tLoss: 2.236425\n",
      "54.75494969165855\n",
      "Train Epoch: 22 [0/12323 (0%)]\tLoss: 1.924876\n",
      "Train Epoch: 22 [3000/12323 (24%)]\tLoss: 2.302481\n",
      "Train Epoch: 22 [6000/12323 (49%)]\tLoss: 2.206989\n",
      "Train Epoch: 22 [9000/12323 (73%)]\tLoss: 2.174421\n",
      "Train Epoch: 22 [12000/12323 (97%)]\tLoss: 2.367598\n",
      "54.62512171372931\n",
      "Train Epoch: 23 [0/12323 (0%)]\tLoss: 2.241123\n",
      "Train Epoch: 23 [3000/12323 (24%)]\tLoss: 1.787690\n",
      "Train Epoch: 23 [6000/12323 (49%)]\tLoss: 2.770188\n",
      "Train Epoch: 23 [9000/12323 (73%)]\tLoss: 1.951578\n",
      "Train Epoch: 23 [12000/12323 (97%)]\tLoss: 2.400508\n",
      "54.78740668614086\n",
      "Train Epoch: 24 [0/12323 (0%)]\tLoss: 1.976076\n",
      "Train Epoch: 24 [3000/12323 (24%)]\tLoss: 2.502207\n",
      "Train Epoch: 24 [6000/12323 (49%)]\tLoss: 2.004754\n",
      "Train Epoch: 24 [9000/12323 (73%)]\tLoss: 2.505554\n",
      "Train Epoch: 24 [12000/12323 (97%)]\tLoss: 2.450375\n",
      "54.04089581304771\n",
      "Train Epoch: 25 [0/12323 (0%)]\tLoss: 2.366204\n",
      "Train Epoch: 25 [3000/12323 (24%)]\tLoss: 2.195663\n",
      "Train Epoch: 25 [6000/12323 (49%)]\tLoss: 2.740182\n",
      "Train Epoch: 25 [9000/12323 (73%)]\tLoss: 1.491563\n",
      "Train Epoch: 25 [12000/12323 (97%)]\tLoss: 1.986117\n",
      "53.521583901330736\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tried by converting the sentences to Tf-Idf vectors by using inbuilt sklearn functions but accuracy was same\n",
    "\n",
    "'''\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#bow_transformer=CountVectorizer(analyzer=text_process).fit(sentences)\n",
    "#BOW=bow_transformer.transform(sentences)\n",
    "#tfidf_transformer=TfidfTransformer().fit(BOW)\n",
    "#BOW=tfidf_transformer.transform(BOW)\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#svd=TruncatedSVD(800)\n",
    "#BOW=svd.fit_transform(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
