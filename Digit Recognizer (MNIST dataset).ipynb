{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digit recognizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Build a deep learning model to predict, which number a hand written digit is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By using Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #nn.module is the parent class for all pytorch models, it has some inbuilt functionalities which calculates gradients\n",
    "    \n",
    "    def __init__(self):# constructor\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        # 1 input layer of 784(28*28 flattened) and a hidden layer with 30 neuron and finally output layer of 10 neurons\n",
    "        self.fc1=nn.Linear(784,30)\n",
    "        self.drop=nn.Dropout(0.2)\n",
    "        self.fc2=nn.Linear(30,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x-shape [64,784] [batchsize,inputsize]\n",
    "        #layer-1\n",
    "        l1=self.fc1(x)# l1-shape [64,30]\n",
    "        \n",
    "        #activation function-1\n",
    "        al1=torch.sigmoid(l1)#al1-shape[64,30]\n",
    "        \n",
    "        #layer-2\n",
    "        l2=self.fc2(al1)#l2-shape [64,10]\n",
    "        \n",
    "        #activation dunction-2\n",
    "        al2=torch.sigmoid(l2)#al2-shape[64,10] which is our output\n",
    "        \n",
    "        return al2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,use_cuda,train_loader,optimizer,epoch):\n",
    "    \n",
    "    model.train()#tell the model to prepare for training\n",
    "    \n",
    "    for batchid,(data,target) in enumerate(train_loader): # get the batch\n",
    "        \n",
    "        #converting the data to [batch_size,784] from [batch_size,1,28,28]\n",
    "        data = data.reshape([data.shape[0], 784]) # [batch_size,784]\n",
    "        \n",
    "        #converting the target to onehot encoding\n",
    "        y_onehot = torch.zeros([target.shape[0], 10])  # Zero vector of shape [batch_size, 10]\n",
    "        y_onehot[range(target.shape[0]), target] = 1 \n",
    "        \n",
    "        if use_cuda:\n",
    "            data,y_onehot=data.cuda(),y_onehot.cuda() #setting the data to GPU if using Gpus\n",
    "            \n",
    "        optimizer.zero_grad()#setting the cumulative gradients to zero\n",
    "        output=model(data)#forward pass through the model\n",
    "        \n",
    "        loss=torch.mean((output-y_onehot)**2)#calculating loss MSE\n",
    "        loss.backward()#calculating gradients of the model\n",
    "        optimizer.step()#updating model parameters . this step doesnt remove stored gradients\n",
    "        \n",
    "        if batchid % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batchid * len(data), len(train_loader.dataset),\n",
    "            100. * batchid / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,use_cuda,test_loader):\n",
    "    \n",
    "    model.eval()#to tell the model to be ready for evaluation\n",
    "    \n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    \n",
    "    with torch.no_grad():#to ensure gradients are not calculated as calculating gradients is not required for testing\n",
    "        for data,target in test_loader:# getting the batch\n",
    "            \n",
    "            #converting the data to [batch_size,784] from [batch_size,1,28,28]\n",
    "            data=data.reshape([data.shape[0],784])\n",
    "            \n",
    "            #converting the target to onehot encoding\n",
    "            y_onehot=torch.zeros([target.shape[0],10])\n",
    "            y_onehot[range(target.shape[0]),target]=1\n",
    "            \n",
    "            if use_cuda:\n",
    "                data,y_onehot=data.cuda(),y_onehot.cuda()\n",
    "                \n",
    "            output=model(data)#forward pass\n",
    "            \n",
    "            test_loss+=torch.sum((output-y_onehot)**2)#sum up batch loss\n",
    "            \n",
    "            pred=output.argmax(dim=1,keepdim=True)#get index of maximum output\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()#getting total number of correct samples\n",
    "            \n",
    "        test_loss/=len(test_loader.dataset)\n",
    "        print(test_loss,100*correct/len(test_loader.dataset))#accuracy=total correct/total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed_value):\n",
    "    #this function removes randomness and makes everything deterministic\n",
    "    #here we set the seed for torch.cuda,torch,numpy and random.\n",
    "    #torch.cuda.manual_seed_all(seed_value) ,if we are using multi-GPU then we should use this to set the seed.\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda=False #if we have GPUs then we can set this true else False and program runs on the cpu\n",
    "    seed(0) #used to fix the randomness of the code.\n",
    "    \n",
    "    #converting the dataset to tensor and normalizing the data by subtracting mean and dividing it by standard deviation,\n",
    "    #normalizing results in the faster converging of model to optimal values.\n",
    "    \n",
    "    # x[0-255] => [0-1]\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,)) #((mean),(std))    (x-mean)/std\n",
    "    ])\n",
    "    \n",
    "    data1=datasets.MNIST('../data',download=True,train=True,transform=transform)#getting the train dataset, shape=[60000,28,28]\n",
    "    data2=datasets.MNIST('../data',train=False,transform=transform)#getting the test dataset. shape=[10000,28,28]\n",
    "    \n",
    "    #loading data using torch's inbuilt dataloader\n",
    "    train_loader=torch.utils.data.DataLoader(data1,num_workers=2,batch_size=64,shuffle=True)#getting train dataloader\n",
    "    test_loader=torch.utils.data.DataLoader(data2,num_workers=2,batch_size=1000,shuffle=False)#getting test dataloader\n",
    "    \n",
    "    model=Net()\n",
    "    \n",
    "    if use_cuda:\n",
    "        model=model.cuda() # if we use GPUs we can put the model weights on GPU \n",
    "        \n",
    "    optimizer=optim.SGD(model.parameters(),lr=10)#choosing the optimizer and setting the learning rate.\n",
    "    \n",
    "    for epoch in range(1,11):\n",
    "        \n",
    "        train(model,use_cuda,train_loader,optimizer,epoch)\n",
    "        test(model,use_cuda,test_loader)\n",
    "        \n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")  #saving the model\n",
    "    model.load_state_dict(torch.load(\"mnist_cnn.pt\"))#loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.285145\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.035109\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.022455\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.016793\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.017698\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.021430\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.014029\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.008732\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.013671\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.016153\n",
      "tensor(0.1293) 92.58\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.008964\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.006504\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.009314\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.004609\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.013835\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.021412\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.010659\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.004104\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.009219\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.005467\n",
      "tensor(0.1072) 93.77\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.007609\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.011521\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.008501\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.015971\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.006641\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.011453\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.008717\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.013753\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.007982\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008468\n",
      "tensor(0.0980) 94.12\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.013763\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.012574\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.016110\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.007436\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.006409\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.012453\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.003608\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007063\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.009318\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.009706\n",
      "tensor(0.0957) 94.33\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007266\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.006465\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.013682\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.007779\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.005149\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.008331\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007827\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.007767\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.006478\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.013309\n",
      "tensor(0.0904) 94.77\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.008112\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.009326\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.008091\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.009440\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.004440\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.003714\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007129\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.004096\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.008722\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.009239\n",
      "tensor(0.0899) 94.59\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.007615\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005865\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.006462\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.008450\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.008097\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.006116\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.004375\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.005383\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.005089\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.004122\n",
      "tensor(0.0862) 94.92\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.009279\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.005073\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.005515\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.004313\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006004\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.007781\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.004134\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.004942\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.012422\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.008297\n",
      "tensor(0.0845) 95.12\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004459\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.006411\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.005159\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.012558\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009532\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.006734\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004614\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.013831\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.007974\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.003389\n",
      "tensor(0.0846) 95.01\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.003765\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.012722\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.004698\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.012681\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.008263\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.004170\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.005476\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.006656\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.004369\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.008720\n",
      "tensor(0.0809) 95.25\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Keras to build a NN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xt,yt),(xte,yte)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xt[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt=xt.reshape(60000,784).astype('float32')\n",
    "xte=xte.reshape(10000,784).astype('float32')\n",
    "xt/=255\n",
    "xte/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10 # number of unique digits\n",
    "\n",
    "Yt= np_utils.to_categorical(yt, nb_classes)\n",
    "Yte = np_utils.to_categorical(yte, nb_classes)\n",
    "yto=np.zeros([yt.shape[0],nb_classes])\n",
    "yteo=np.zeros([yte.shape[0],nb_classes])\n",
    "for i in range(yt.shape[0]):\n",
    "    yto[i][yt[i]]=1\n",
    "for i in range(yte.shape[0]):\n",
    "    yteo[i][yte[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()                        #Linear stacking of layers\n",
    "\n",
    "model.add(Dense(512,input_shape=(784,)))  #First layer with 512 nodes\n",
    "model.add(Activation('relu'))             #Relu activation function \n",
    "\n",
    "model.add(Dense(50))                      #second layer with 50 nodes\n",
    "model.add(Activation('relu'))             #relu activation layer\n",
    "model.add(Dropout(0.2))                   #20% dropout of randomly selected nodes\n",
    "\n",
    "model.add(Dense(10))                      #Final layer with 10 nodes and\n",
    "model.add(Activation('softmax'))          #softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                25650     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 428,080\n",
      "Trainable params: 428,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 0.0186 - accuracy: 0.9937\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0115 - accuracy: 0.9963\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0125 - accuracy: 0.9958\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0131 - accuracy: 0.9956\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0125 - accuracy: 0.9961\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0077 - accuracy: 0.9974\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0099 - accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0101 - accuracy: 0.9969\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0080 - accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d7a21ad208>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=xt,y=yto,batch_size=64,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9832\n",
      "Test score: 0.0952552855014801\n",
      "Test accuracy: 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(xte,yteo)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding convolution and maxpooling layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape (60000, 28, 28, 1)\n",
      "Testing matrix shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')         # change integers to 32-bit floating point numbers\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255                              # normalize each value for each pixel for the entire vector for each input\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10 # number of unique digits\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()                                 # Linear stacking of layers\n",
    "\n",
    "# Convolution Layer 1\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "convLayer01 = Activation('relu')                     # activation\n",
    "model.add(convLayer01)\n",
    "\n",
    "# Convolution Layer 2\n",
    "model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "model.add(Activation('relu'))                        # activation\n",
    "convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
    "model.add(convLayer02)\n",
    "\n",
    "# Convolution Layer 3\n",
    "model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "convLayer03 = Activation('relu')                     # activation\n",
    "model.add(convLayer03)\n",
    "\n",
    "# Convolution Layer 4\n",
    "model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps\n",
    "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
    "model.add(Activation('relu'))                        # activation\n",
    "convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
    "model.add(convLayer04)\n",
    "model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector\n",
    "\n",
    "# Fully Connected Layer 5\n",
    "model.add(Dense(512))                                # 512 FCN nodes\n",
    "model.add(BatchNormalization())                      # normalization\n",
    "model.add(Activation('relu'))                        # activation\n",
    "\n",
    "# Fully Connected Layer 6                       \n",
    "model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes\n",
    "model.add(Dense(10))                                 # final 10 FCN nodes\n",
    "model.add(Activation('softmax'))                     # softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 597,738\n",
      "Trainable params: 596,330\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = gen.flow(X_train, Y_train, batch_size=128)\n",
    "test_generator = test_gen.flow(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "468/468 [==============================] - 82s 164ms/step - loss: 0.2856 - accuracy: 0.9107 - val_loss: 0.0676 - val_accuracy: 0.9798\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 80s 171ms/step - loss: 0.0558 - accuracy: 0.9818 - val_loss: 0.0519 - val_accuracy: 0.9830\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 82s 175ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 0.0654 - val_accuracy: 0.9772\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 84s 180ms/step - loss: 0.0318 - accuracy: 0.9898 - val_loss: 0.0244 - val_accuracy: 0.9916\n",
      "Epoch 5/5\n",
      "468/468 [==============================] - 82s 176ms/step - loss: 0.0303 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d7a2a9a3c8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=60000//128, epochs=5, verbose=1, \n",
    "                    validation_data=test_generator, validation_steps=10000//128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 10ms/step - loss: 0.0301 - accuracy: 0.9912\n",
      "Test score: 0.030063379555940628\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
